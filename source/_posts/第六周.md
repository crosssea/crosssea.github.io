---
title: 第六周
top: true
cover: false
toc: true
mathjax: true
date: 2020-10-29 12:37:45
password:
summary:
tags:
- 克盐杂记
- AttentionModule
categories:
- 论文阅读
---

> 论文看的不多
>
> 感觉这方面的东西以前仅限于听过，还是要多了解一下

### 一. Convolutional TripletAttention Module



### Convolutional TripletAttention Module

<img src=".\source\pic1.png" style="zoom:67%;" />

<p>
    	作者指出以往的Attention机制的计算方法有许多的不足和缺陷，例如计算复杂度太高，忽略了空间和通道
    之间的信息的相互之间的相关性等。
    	提出了一种参数量极少，却效率极高的三路并行的Attention Module。
</p>



#### 1.1 Channel Attention in CBAM

>$\chi \in R^{C \times H \times W}$是卷积之后的输出也就是CBAM的输入
>
>求得权重     $\omega = \sigma(f_{(w_0,w_1)}(g(\chi)) +f_{(w_0,w_1)}(\delta(\chi)))  $
>
>其中 $g$ 代表全局平均池化， $ f $代表全局最大池化 得到的 $w \in R^{C \times 1 \times 1}$

<p>虽然考虑了通道和空间信息，但是却没有考虑两者之间的联系</p>

#### 1.2 Triplet Attention

结构

<img src=".\source\pic2.png" style="zoom:67%;" />

> In simple terms, the spatial attention tells  ’where in the channel to focus’  and the channel attention tells  ’what channel to focus on’

<p>
    作者指出传统的Attention的池化操作没有考虑到空间上的信息，而CBAM虽然使用全局平均池化来补充这个信息
    却忽视了两者之间的联系，于是提出三路并行的Triplet Attention来捕捉 C,H,W 两两之间的信息
</p>


##### 1.2.1 Z-pool

​	$$Z-pool(\chi) = [MaxPool_{0d}(\chi),AvgPool_{0d}(\chi)]$$

>将最大值池化和平均池化的特征图拼接起来
>其中 $0d$ 表示池化操作所在的维度是第0维



##### 1.2.2 每个branch的操作过程

* 相对某个维度进行旋转 (说的很玄乎，貌似就是一个转置)
* 利用两种池化消去第一个维度之后拼接，作者指出这样可以保留剩余两个维度之间的相关关系
* 经过卷积与激活函数之后与旋转之后的数据相乘
* 再将数据旋转回去
* 对三个分支的数据进行取平均值

##### 1.2.3 参数分析和实验

<img src=".\source\pic3.png" style="zoom:100%;" />

<img src=".\source\pic5.png" alt="分类" style="zoom:67%;" />

<img src=".\source\pic6.png" alt="目标检测" style="zoom:67%;" />



